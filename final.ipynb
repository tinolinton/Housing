{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a58f56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2eda24e",
   "metadata": {},
   "source": [
    "### a) Key Stages to Build and Deploy a Deep Learning Model for Property Price Prediction\n",
    "## 1.\tProblem Definition\n",
    "### Objective: We would clearly articulate the goal of predicting property prices based on various features. \n",
    "### This includes identifying the target audience and the specific requirements of the prediction task, such as accuracy \n",
    "### and interpretability.\n",
    "## Data Collection\n",
    "### Objective: We would gather relevant datasets that include features such as location, size, number of bedrooms, \n",
    "### and historical prices. High-quality data is crucial for training an effective model.\n",
    "## Data Preprocessing\n",
    "### Objective: We would clean and prepare the data for analysis. This involves handling missing values, encoding \n",
    "### categorical variables, and normalizing numerical features to ensure the data is suitable for deep learning.\n",
    "## Exploratory Data Analysis\n",
    "### Objective: We would analyze the dataset to understand feature distributions and relationships. This helps us \n",
    "### identify trends, correlations, and potential outliers that could affect model performance.\n",
    "## Feature Engineering\n",
    "### Objective: We would create new features or transform existing ones to enhance model performance. This may \n",
    "### involve generating interaction terms or aggregating features that better capture the underlying patterns in the data.\n",
    "## Model Selection\n",
    "### Objective: We would choose an appropriate deep learning architecture based on the nature of the data and task \n",
    "### complexity.\n",
    "## Model Training\n",
    "### Objective: We would train the selected model using the training dataset. This involves feeding the data through \n",
    "### the model and adjusting weights based on the loss function.\n",
    "## Model Evaluation\n",
    "### Objective: We would assess the model's performance using metrics such as Mean Absolute Error and R-squared on a \n",
    "### separate validation dataset to ensure it generalizes well.\n",
    "## Model Tuning\n",
    "### Objective: We would optimize hyperparameters and model architecture to enhance performance. This includes \n",
    "### adjusting learning rates, batch sizes, and the number of layers or neurons.\n",
    "## Deployment\n",
    "### Objective: We would deploy the trained model for real-world use, making it accessible through a web application or API. This allows end-users to input features and receive price predictions.\n",
    "## Monitoring and Maintenance\n",
    "### Objective: We would continuously monitor the model's performance after deployment, ensuring it remains accurate \n",
    "### as new data comes in. This may involve regular retraining with updated datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a08da54",
   "metadata": {},
   "source": [
    "## b) Critical Hyperparameters to Tune\n",
    "## 1.Learning Rate\n",
    "### The learning rate controls how much we change the model in response to the estimated error each time we \n",
    "### update the model weights. A learning rate that is too high may cause the model to converge too quickly to a \n",
    "### suboptimal solution. A rate that is too low may slow down the training process.\n",
    "### Proper tuning of the learning rate is essential to ensure effective training, allowing the model to learn \n",
    "### efficiently without overshooting the optimal solution.\n",
    "## 2.Batch Size\n",
    "### The batch size determines how many samples we process before updating the model's internal parameters. \n",
    "### Smaller batch sizes provide a more detailed gradient update, while larger sizes can speed up training.\n",
    "### Tuning batch size can impact convergence speed and model performance, influencing both the stability of the \n",
    "### training process and the final accuracy of the model.\n",
    "## 3.Number of Layers and Neurons\n",
    "### The architecture of the neural network, including the number of layers and neurons in each layer, defines the model's capacity to learn complex patterns. More layers can capture more intricate relationships but may also lead to overfitting.\n",
    "### Adjusting the depth and width of the model is critical in balancing underfitting and overfitting, ultimately impacting the model's ability to generalize to unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ebc28c",
   "metadata": {},
   "source": [
    "## c) Challenges and Strategies\n",
    "### 1.Data Quality and Quantity\n",
    "### We might face issues with insufficient or poor-quality data, leading to inaccurate predictions and a lack of \n",
    "### model robustness. Real estate data can be noisy and may contain missing or inconsistent entries.\n",
    "### Strategy: We would implement robust data cleaning and preprocessing techniques to handle missing values and outliers. Additionally, we might augment the dataset by sourcing more data from multiple platforms or using synthetic data generation methods.\n",
    "## 2.Model Interpretability\n",
    "### Deep learning models are often seen as black boxes, making it difficult for us to interpret how predictions \n",
    "### are made. This can be a barrier in industries like real estate, where stakeholders need to understand \n",
    "### the reasoning behind price predictions.\n",
    "### Strategy: We could use techniques such as SHAP or LIME to provide insights into feature contributions. \n",
    "### This can enhance trust and transparency in the model's predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c29d087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from kerastuner import HyperModel, RandomSearch\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8a6b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the dataset\n",
    "data = pd.read_csv('Housing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4e22cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting all variable names\n",
    "data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6e57c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4555e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "data.head()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4485aba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "\"Missing Values:\\n\", missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a18f798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill numeric columns with their mean\n",
    "numeric_columns = data.select_dtypes(include=['number']).columns\n",
    "data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].mean())\n",
    "\n",
    "# Fill categorical columns with their mode\n",
    "categorical_columns = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea', 'furnishingstatus']\n",
    "for col in categorical_columns:\n",
    "    data[col].fillna(data[col].mode()[0], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d971cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = data.duplicated().sum()\n",
    "f'Duplicate Rows: {duplicates}'\n",
    "data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be522854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert relevant columns to categorical\n",
    "for col in categorical_columns:\n",
    "    data[col] = data[col].astype('category')\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "data = pd.get_dummies(data, drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7bc5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Handle Outliers using IQR\n",
    "Q1 = data['price'].quantile(0.25)\n",
    "Q3 = data['price'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define outlier bounds\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3980586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "data = data[(data['price'] >= lower_bound) & (data['price'] <= upper_bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842a0e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize After Removing Outliers\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(y=data['price'])\n",
    "plt.title('Box Plot of House Prices (After Outlier Removal)')\n",
    "plt.ylabel('Price')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfb157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Data Visualization\n",
    "# Distribution of house prices\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data['price'], bins=30, kde=True)\n",
    "plt.title('Distribution of House Prices')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eefe934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting figure size\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plotting the countplot for the number of bedrooms\n",
    "ax = sns.countplot(x='bedrooms', data=data)  # Use 'data' as the DataFrame\n",
    "plt.xlabel(\"Number of Bedrooms\")  \n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Data Distribution of Number of Bedrooms\")\n",
    "\n",
    "# Adding labels to each bar\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i,)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dec8b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a count plot for a given feature\n",
    "def plot_count_distribution(data, feature):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    ax = sns.countplot(x=feature, data=data)\n",
    "    plt.xlabel(f\"Number of {feature.capitalize()}\")  \n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(f\"Data Distribution of Number of {feature.capitalize()}\")\n",
    "\n",
    "    # Adding labels to each bar\n",
    "    for i in ax.containers:\n",
    "        ax.bar_label(i,)\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "# Plot for number of bedrooms\n",
    "plot_count_distribution(data, 'bedrooms')\n",
    "\n",
    "# Plot for number of bathrooms\n",
    "plot_count_distribution(data, 'bathrooms')\n",
    "\n",
    "# Plot for number of stories\n",
    "plot_count_distribution(data, 'stories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d2ca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a count plot for a given feature\n",
    "def plot_count_distribution(data, feature):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    ax = sns.countplot(x=feature, data=data)\n",
    "    plt.xlabel(f\"{feature.capitalize()}\")  \n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(f\"Data Distribution of {feature.capitalize()}\")\n",
    "\n",
    "    # Adding labels to each bar\n",
    "    for i in ax.containers:\n",
    "        ax.bar_label(i,)\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "# Plot for parking\n",
    "plot_count_distribution(data, 'parking')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32709d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap for correlation matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "correlation_matrix = data.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', square=True, cbar_kws={\"shrink\": .8})\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af57f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "print(\"\\nMissing values before cleaning:\")\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ea8e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5157fd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical feature\n",
    "data = pd.get_dummies(data, columns=['furnishingstatus'], drop_first=True)\n",
    "\n",
    "# Split data\n",
    "X = data.drop('price', axis=1)\n",
    "y = data['price']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
